\documentclass{article}

\usepackage{hyperref}
%\usepackage{fullpage}
\usepackage{bm}

\usepackage[T1]{fontenc}
\usepackage[sc]{mathpazo}
\linespread{1.05}         % Palatino needs more leading (space between lines)

\usepackage[american]{babel}
\usepackage{csquotes}
\usepackage[style=apa,natbib=true,backend=biber]{biblatex}
%\usepackage[longnamesfirst]{natbib}
\DeclareLanguageMapping{american}{american-apa}


\bibliography{lab}

\title{Supplement to ``The Fallacy of Placing Confidence in Confidence Intervals''}
\author{Richard D. Morey (\href{mailto:richarddmorey@gmail.com}{richarddmorey@gmail.com}),\\ Rink Hoekstra, Jeffrey N. Rouder,\\ Michael D. Lee, Eric-Jan Wagenmakers}
\date{\today}
\begin{document}

\maketitle
<<echo=FALSE,message=FALSE,warning=FALSE,results='hide'>>=
library(xtable)
set.seed(1)
@
\section{The lost submarine: details}\label{app:submarine}

We presented a situation where $N=2$ observations were distributed uniformly:

\begin{eqnarray*}
x_i &\stackrel{iid}{\sim}& \mbox{Uniform}(\theta-5,\theta+5),\,i=1,\ldots,N 
\end{eqnarray*}
and the goal is to estimate $\theta$, the location of the submarine hatch. Without loss of generality we denote $x_1$ as the smaller of the two observations. In the text, we considered five 50\% confidence procedures; in this section, we give the details about the sampling distribution procedure and the Bayes procedure that were omitted from the text.


\subsection{Sampling distribution procedure}

Consider the sample mean, $\bar{x} = (x_1 + x_2)/2$. As the sum of two uniform deviates, it is a well-known fact that $\bar{x}$ will have a triangular distribution with location $\theta$ and minimum and maximum $\theta-5$ and $\theta+5$, respectively. This distribution is shown in Figure~\ref{fig:sampleDistCI}.

<<sampleDistCI,echo=FALSE,fig.cap="The sampling distribution of the mean $\\bar{x}$ in the submarine scenario. The shaded region represents the central 50\\% of the area. The unshaded triangle marked ``a'' has area .25.",fig.width=8,fig.height=6>>=

par(mfrow=c(1,1), mar = c(4,4,2,1),mgp=c(3,.6,0), las = 1, cex.axis = 1.3, cex.lab = 1.3, cex = 1.3)

plot(0,0,ty='n',ylim=c(0,2),xlim=c(-.5,1.5),axes=F,ylab="Density",xlab="Location")
axis(1,at=1:5/2-1,lab=expression(theta - 10,
                                 theta - 5,
                                 theta,
                                 theta + 5,
                                 theta + 10
))
box()
axis(2, at = 0:4/2, lab = 0:4/20)
abline(h=0,col="gray")
segments(-2,0,0,0, lwd=2)
segments(1,0,2,0, lwd=2)
segments(0,0,.5,2, lwd=2)
segments(.5,2,1,0, lwd=2)

ci.hwidth = (1-1/sqrt(2))/2

x.pts = c(.5 - ci.hwidth, .5 - ci.hwidth, .5, .5 + ci.hwidth, .5 + ci.hwidth)
y.pts = c(0,(.5-ci.hwidth)*4,2,(.5-ci.hwidth)*4,0)

polygon(x.pts,y.pts, lty=0, col=rgb(0,1,0,.2))

text(.23,1.5/5,"a")

@

It is desired to find the width of the base of the shaded triangle in Figure~\ref{fig:sampleDistCI} such that it has an area of .5. To do this we first find the width of the base of the unshaded triangular area marked ``a'' in Figure~\ref{fig:sampleDistCI} such that the area of the triangle is .25. The corresponding unshaded triangle on the left side will also have area .25, which means that since the figure is a density, the shaded region must have the remaining area of .5. Elementary geometry will show that the width of the base of triangle ``a'' is $5/\sqrt{2}$, meaning that the distance between $\theta$ and the altitude of triangle ``a'' is $5 - 5/\sqrt{2}$ or about 1.46m.

We can thus say that 

\[
Pr(- (5 - 5/\sqrt{2}) < \bar{x} - \theta <  5 - 5/\sqrt{2} ) = .5
\]

which implies that, in repeated sampling,

\[
Pr(\bar{x} - (5 - 5/\sqrt{2}) < \theta <  \bar{x} + (5 - 5/\sqrt{2}) ) = .5
\]

which defines the sampling distribution confidence procedure. This is an example of using $\bar{x} - \theta$ as a pivotal quantity \citep{Casella:Berger:2002}.

\subsection{Bayesian procedure}

The posterior distribution is proportional to the likelihood times the prior. The likelihood is
\[
p(x_1,x_2\mid\theta) \propto \prod_{i=1}^2 {\cal I}(\theta-5 < x_i < \theta+5);
\]
where $\cal I$ is an indicator function. Note since this is the product of two indicator functions, it can only be nonzero when both indicator functions' conditions are met; that is, when $x_1+5$ and $x_2+5$ are both greater than $\theta$, and $x_1-5$ and $x_2-5$ are both less than $\theta$. If the minimum of $x_1+5$ and $x_2+5$ is greater than $\theta$, then so to must be the maximum. The likelihood thus can be rewritten
\[
p(x_1,x_2\mid\theta) \propto {\cal I}(x_2 - 5 <\theta< x_1+5);
\]
where $x_1$ and $x_2$ are the minimum and maximum observations, respectively. If the prior for $\theta$ is proportional to a constant, then the posterior is
\[
p(\theta\mid x_1,x_2) \propto {\cal I}(x_2 - 5 <\theta< x_1+5),
\]
This posterior is a uniform distribution over all {\em a posteriori} possible values of $\theta$ (that is, all $\theta$ values within 5 meters of all observations), has width 
\[
10 - (x_{2} - x_{1}),
\]
and is centered around $\bar{x}$. Because the posterior comprises all values of $\theta$ the data have not ruled out -- and is essentially just the classical likelihood -- the width of this posterior can be taken as an indicator of the precision of the estimate of $\theta$. 

The middle 50\% of the likelihood can be taken as a 50\% objective Bayesian credible interval. Proof that this Bayesian procedure is also a confidence procedure is trivial and can be found in \citet{Welch:1939}.

\section{Credible interval for $\omega^2$: details}

In the manuscript, we compare Steiger's (2004) confidence intervals for $\omega^2$ to Bayesian highest posterior density (HPD) credible intervals. In this section we describe how the Bayesian HPD intervals were computed.

\nocite{Steiger:2004}

Consider a one-way design with $J$ groups and $N$ observations in each group. Let $y_{ij}$ be the $i$th observation in the $j$th group. Also suppose that
\[
y_{ij} \stackrel{indep.}{\sim} \mbox{Normal}(\mu_j, \sigma^2)
\]
where $\mu_j$ is the population mean of the $j$th group and $\sigma^2$ is the error variance. We assume a ``non-informative'' prior on parameters $\bm \mu,\sigma^2$:
\[
p(\mu_1,\ldots,\mu_J,\sigma^2) \propto (\sigma^2)^{-1}.
\]
This prior is flat on $(\mu_1,\ldots,\mu_J, \log\sigma^2)$. In application, it would be wiser to assume an informative prior on these parameters, in particular assuming a population over the $\mu$ parameters or even the possibility that $\mu_1 = \ldots = \mu_J = 0$ \citep{Rouder:etal:2012}. However, for this manuscript we compare against a ``non-informative'' prior in order to show the differences between the confidence interval and the Bayesian result with ``objective'' priors. 

Assuming the prior above, an elementary Bayesian calculation \citep{Gelman:etal:2004} reveals that
\[
\sigma^2\mid\bm y \sim \mbox{Inverse Gamma}(J(N-1)/2, S/2)
\]
where $S$ is the error sum-of-squares from the corresponding one-way ANOVA, and
\[
\mu_j\mid\sigma^2, \bm y \stackrel{indep.}{\sim} \mbox{Normal}(\bar{x}_j, \sigma^2/N)
\]
where $\mu_j$ and $\bar{x}_j$ are the true and observed means for the $j$th group. Following Steiger (2004) we can define 
\[
\alpha_j = \mu_j - \frac{1}{J}\sum_{j=1}^J\mu_j
\]
as the deviation from the grand mean of the $j$th group, and
\begin{eqnarray*}
\lambda &=& N\sum_{j=1}^J \left(\frac{\alpha}{\sigma}\right)^2\\
\omega^2 &=& \frac{\lambda}{\lambda + NJ}.
\end{eqnarray*}

It is then straightforward to set up an MCMC sampler for $\omega^2$. Let $M$ be the number of MCMC iterations desired. We first sample $M$ samples from the marginal posterior distribution of $\sigma^2$, then sample the group means from the conditional posterior distribution for $\mu_1,\ldots,\mu_J$. Using these posterior samples, $M$ posterior samples for $\lambda$ and $\omega^2$ can be computed. 

<<echo=FALSE,results='hide',message=FALSE>>=
source('steiger.utility.R')

@

The following function will sample from the marginal posterior distribution of $\omega^2$:

<<>>=
## Assumes that data.frame y has two columns:
## $y is the dependent variable
## $grp is the grouping variable, as a factor
Bayes.posterior.omega2
@

The {\tt Bayes.posterior.omega2} function can be used to compute the posterior and HPD for the first example in the manuscript. The {\tt fake.data.F} function, defined in the file {\tt steiger.utility.R} (available with the manuscript source code at \url{https://github.com/richarddmorey/ConfidenceIntervalsFallacy}), generates a data set with a specified $F$ statistic.

<<>>=
cl = .683 ## Confidence level corresponding to standard error
J = 3 ## Number of groups
N = 10 ## observations in a group

df1 = J - 1
df2 = J * (N - 1)

## F statistic from manuscript
Fstat = 0.1748638

set.seed(1)
y = fake.data.F(Fstat, df1, df2)

## Steiger confidence interval
steigerCI = steigerCI.omega2(Fstat,df1,df2, conf.level=cl)
samples.omega2 = Bayes.posterior.omega2(y, cl, 100000)
@

We can compute the Bayesian HPD interval with the `HPDinterval` function in the package `coda`: 
<<>>=
library(coda)

HPDinterval( samples.omega2, prob = cl )
@

<<omega2samp,echo=FALSE,fig.cap="Histogram of the posterior MCMC samples for $\\omega^2$. The 68\\% Bayesian HPD credible interval is highest density region than captures 68\\% of the posterior density, shown in gray.  The vertical dashed line denotes the upper bound of the HPD. The 68\\% Steiger confidence interval is shown as the interval near the top.",fig.width=8,fig.height=6>>=

nbreaks = 20

q = HPDinterval( samples.omega2, prob = cl )
bks = seq(0,max(samples.omega2)*1.01,q[2]/nbreaks)
cols = rgb(1-(bks<q[2]),0,0,.3)

z = hist(samples.omega2, breaks = bks, main=expression(paste("Posterior distribution for ",omega^2)), freq=FALSE, col=cols,lty=0, xlab = expression(omega^2))
abline(v=q[2], lty=2)
text(q[2]/2,3,"68%", cex = 2)

arrows(steigerCI[1], par()$usr[4]*.75, steigerCI[2]+.0005,par()$usr[4]*.75, col="red", lwd=2, code = 3, angle = 90)

@



\printbibliography

\end{document}



